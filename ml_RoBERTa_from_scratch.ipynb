{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72d4e093-90d5-46dc-a2e7-b1ceb6dec1fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "In this notebook we pretrain a [RoBERTa](https://huggingface.co/transformers/model_doc/roberta.html#) model from scratch on Danish using huggingface. RoBERTa is a slightly optimized version of BERT with a tiny embedding tweak. Furthermore RoBERTa uses a byte-level BPE as a tokenizer.\n",
    "\n",
    "This notebook is based on the following [huggingface notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=UIvgZ3S6AO0z).  \n",
    "\n",
    "The following steps are carried out:  \n",
    "1. Setting Up Workspace  \n",
    "2. Preparing Data  \n",
    "3. Training BPE Tokenizer  \n",
    "4. Configuring RoBERTa  \n",
    "5. Training RoBERTa  \n",
    "6. Testing Trained RoBERTa  \n",
    "  \n",
    "Links:  \n",
    "RoBERTa Paper: https://arxiv.org/abs/1907.11692  \n",
    "RoBERTa Huggingface: https://huggingface.co/transformers/model_doc/roberta.html#  \n",
    "RoBERTa Blog Post: https://huggingface.co/blog/how-to-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05c1b467-d93d-452a-83f1-3d07f75bb6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1. Setting Up Workspace\n",
    "It is recommended to install transformers directly from github as the pip version is usually outdated, and thus misses several important features such as resuming from a checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "03d0f163-7375-4144-b14b-8afac9e36f2e",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install transformers from master\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4eae46d9-5bc0-4b7b-86b1-670c9c473f01",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting versions  \n",
    "!pip list | grep -E 'transformers|tokenizers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e40422a2-5378-44ee-828b-a24b92dec214",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e7ee4b5-e2e5-4247-bad0-89cc1d3bc9fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9398040-c4a9-48e1-a2db-dc918a7582e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking that PyTorch sees GPU\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "620cb30e-54be-4f0c-bbc6-5f5e35e5ef29",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Preparing data\n",
    "We will be using the Danish wikipedia for training our model and hopefully later the [Danish Gigaword Corpus](https://gigaword.dk/).\n",
    "  \n",
    "The training data needs to be in the one line pr document, which is achieved using the following steps:  \n",
    "1. Download Danish wikipedia  \n",
    "2. Clean XML file\n",
    "3. Divided file into one article pr file  \n",
    "4. Concatenated articles into train, valid and test files (Articles are scrambled and concatenated where each article is separated by an empty line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ac2305d-81a6-49d4-a2b0-bf31c1ef05e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Downloading danish wikipedia\n",
    "!curl -O -J -L https://dumps.wikimedia.org/dawiki/20210401/dawiki-20210401-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpacking file\n",
    "!bzip2 -d dawiki-20210401-pages-articles.xml.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing wikiextractor to clean xml\n",
    "!git clone https://github.com/attardi/wikiextractor.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikiextractor has some problems with its imports and the following sed command fixes this:  \n",
    "\n",
    "**Before:**  \n",
    "from .extract import Extractor, ignoreTag, define_template, acceptedNamespaces  \n",
    "  \n",
    "**After:**  \n",
    "from extract import Extractor, ignoreTag, define_template, acceptedNamespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing first occurence of .extract with extract\n",
    "!sed -i '0,/.extract/{s/.extract/extract/}' ./wikiextractor/wikiextractor/WikiExtractor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting articles to one big file\n",
    "!cd wikiextractor/wikiextractor && python3 WikiExtractor.py ../../dawiki-20210401-pages-articles.xml --no-templates --processes 4 -b 100G -o ../../ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving files around and cleaning up\n",
    "!mv AA/wiki_00 wiki_da.txt \n",
    "!rm -r AA dawiki-20210401-pages-articles.xml wikiextractor\n",
    "!mkdir wikipedia_da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create one file pr article using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki = open(\"wiki_da.txt\")\n",
    "dest = \"wikipedia_da/\"\n",
    "f = None\n",
    "for i,l in enumerate(wiki):\n",
    "    if i%10000 == 0:\n",
    "        print(str(i))\n",
    "    title_re = re.compile(rf'<doc id=\"\\d+\" url=\"https://da.wikipedia.org/wiki\\?curid=\\d+\" title=\"([^\"]+)\">')\n",
    "    if l.startswith('<doc id=\"'):\n",
    "        title = title_re.findall(l)[0].replace('/','_').replace(\"'\", \"\").replace('\"', '')\n",
    "        if len(title)>150: continue\n",
    "        if f: f.close()\n",
    "        f = open(dest + title.replace(' ','_') + '.txt', 'w')\n",
    "    if l.startswith('</doc>'):\n",
    "        continue\n",
    "    if not l:\n",
    "        f.write('')\n",
    "    else:\n",
    "        f.write(l)\n",
    "f.close()\n",
    "wiki.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing articles and scrambling\n",
    "article_paths = glob.glob('wikipedia_da/*.txt')\n",
    "random.shuffle(article_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing into train, valid and test (90 %, 5 %, 5 %)\n",
    "train, valid, test = numpy.split(article_paths, [int(0.90*len(article_paths)),int(0.95*len(article_paths))]); print(str(len(train)) + \" \" + str(len(valid)) + \" \" + str(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing train to file\n",
    "with open('wiki_train.txt', 'w') as out_file:\n",
    "    for file_path in train:\n",
    "        with open(file_path) as in_file:\n",
    "            # replacing line endings with whitespace\n",
    "            lines = \" \".join([l[:-1] + \" \" for l in in_file.readlines()[1:]])\n",
    "            out_file.write(lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing valid to file\n",
    "with open('wiki_valid.txt', 'w') as out_file:\n",
    "    for file_path in valid:\n",
    "        with open(file_path) as in_file:\n",
    "            # replacing line endings with whitespace\n",
    "            lines = \" \".join([l[:-1] + \" \" for l in in_file.readlines()[1:]])\n",
    "            out_file.write(lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing test to file\n",
    "with open('wiki_test.txt', 'w') as out_file:\n",
    "    for file_path in test:\n",
    "        with open(file_path) as in_file:\n",
    "            # replacing line endings with whitespace\n",
    "            lines = \" \".join([l[:-1] + \" \" for l in in_file.readlines()[1:]])\n",
    "            out_file.write(lines + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8602a085-aec1-465b-b11c-a0b2e8f79609",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Training BPE Tokenizer\n",
    "Training a byte-level BPE tokenizer is prefered to the WordPiece tokenizer of BERT because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens, which means that we for the most part limit the use of ```<unk>``` tokens. The special tokens are set in the order required by RoBERTa.\n",
    "\n",
    "Training a Byte-Pair Encoding (BPE) tokenizer follows the following process:  \n",
    "1. Start with all the characters present in the training corpus as tokens.\n",
    "2. Identify the most common pair of tokens and merge it into one token.\n",
    "3. Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.\n",
    "\n",
    "Link: https://huggingface.co/docs/tokenizers/python/latest/quicktour.html#build-a-tokenizer-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d87319de-299d-45b2-affe-ac9457a735f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Listing files\n",
    "paths = ['wiki_train.txt', 'wiki_valid.txt', 'wiki_test.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3703a138-d4aa-41f3-a9d3-8aa066da81a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a45ec2d-936c-4cf8-a04b-7bc913c00ce1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=25_000, min_frequency=3, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa5da94f-f026-4609-afc6-eeaf9f7193a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving model\n",
    "os.makedirs(\"DaBERTo\", exist_ok=True)\n",
    "tokenizer.save_model(\"DaBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec55075e-d8d6-4366-9bd0-b97594bc76b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Testing the tokenizer\n",
    "tokenizer = RobertaTokenizerFast('DaBERTo/vocab.json', 'DaBERTo/merges.txt')\n",
    "ids = tokenizer.encode(\"Mit navn er Dronning Magrethe!\")\n",
    "tokens = tokenizer.batch_decode(ids)\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "print(\"IDs:\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0a1695c1-a729-412b-a55f-2532d54a1451",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now have both a __vocab.json__, which is a list of the most frequent tokens ranked by frequency, and a __merges.txt__ list of merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00b55349-d369-4217-a37c-c70c641567ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Configuring RoBERTa \n",
    "Here we configure the model for the task of Masked language modeling, i.e. to predict how to fill arbitrary tokens that we randomly mask in the dataset. As we are training from scratch, we only initialize from a config, and not from an existing pretrained model or checkpoint. The model automatically takes care of the masking during training. We keep the model very close to the default model, thus we just set the vocabulary size from our tokenizer, and we ensure a context window of 512 (max_position_embeddings includes start/end tokens, thus +2)  \n",
    "\n",
    "Notes on GPU Memory and CUDA errors:  \n",
    "* If your **chunk_size** is too large compared to the max_position_embeddings, you will get errors such as:\n",
    "  * cuda error: CUBLAS_STATUS_NOT_INITIALIZED\n",
    "  * cuda error device-side assert triggered\n",
    "  * cuda error cublas_status_alloc_failed when calling cublascreate(handle)\n",
    "* If you have not balanced **vocab_size**, **num_hidden_layers** and **chunk_size** well, you will get errors such as:\n",
    "  * cuda out of memory. tried to allocate 20.00 mib....\n",
    "\n",
    "To solve the first error, you have to lower the **chunk_size**, and to resolve the second case you should lower one or all of the mentioned parameres, or get a GPU with more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2adfc3d-9e43-4550-9df9-71d08e3f895b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./DaBERTo\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71964a4b-a77e-48b2-a8fd-9f8f69905d57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=25_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47141aca-6e77-47e3-8155-d9e7876ceb21",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1b0fc33a-2474-49d9-8daa-26c0973f9b96",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we write a custom dataset class that uses our pretrained tokenizer. The class further more cuts the text corpus into chunks of 510 tokens and adds the special tokens ```<s>``` and ```</s>``` to the start and end of the sequences. We furthermore throw away the final sequence instead of padding it to 512 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6e2b3eb6-f4ad-40f2-a51b-9c5474a6747e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ChunkedTextDataset(Dataset):\n",
    "    def __init__(self, tokenizer_path: str, file_paths: list, chunk_size=510):\n",
    "        tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path, chunk_size=chunk_size)\n",
    "        all_text = self.group_texts(file_paths)\n",
    "        chunks = self.chunker(all_text, 1000)\n",
    "        all_tokens = []\n",
    "        for chunk in chunks:\n",
    "            all_tokens.extend(tokenizer.encode('\\n'.join(chunk))[1:-1])\n",
    "        self.chunks = [{\"input_ids\":torch.tensor(chunk, dtype=torch.long)} for chunk in self.chunker_add_seq_tokens(all_tokens, chunk_size)[:-1]]\n",
    "    \n",
    "    @staticmethod\n",
    "    def read_file(path):\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "        return lines\n",
    "      \n",
    "    def group_texts(self, list_of_paths):\n",
    "        # Concatenating all texts\n",
    "        all_text = []\n",
    "        for path in list_of_paths:\n",
    "            all_text.extend(self.read_file(path))\n",
    "        return all_text\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunker_add_seq_tokens(seq, size):\n",
    "        chunks = [seq[pos:pos + size] for pos in range(0, len(seq), size)]\n",
    "        for chunk in chunks:\n",
    "            chunk.insert(0,0)\n",
    "            chunk.insert(len(chunk),2)\n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.chunks)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.chunks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f30e9eb-ef40-42ae-996d-ddb3faf00842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = ChunkedTextDataset(\n",
    "    tokenizer_path='./DaBERTo/', \n",
    "    file_paths=[\"./wiki_train.txt\"],\n",
    "    chunk_size=510,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e0eacc28-6e89-428b-8a45-68087b4c25a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b172f57e-f392-471a-9d58-b230fbd6379f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./DaBERTo\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_gpu_train_batch_size=20,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2c23fbdc-76c1-4339-9bc2-e3a4fae80a2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "00861bfa-60af-482b-9d8f-8067212567a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5. Training RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a89429f3-f50f-4af2-bca4-fb84b6f81ed7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d8306f9-7bd1-4095-a531-46ec2ce6f425",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6. Testing Trained RoBERTa\n",
    "We are testing whether RoBERTa can fill in the ```<mask>``` in a sensible way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0b26efa3-1ac6-4311-8a61-970be1b54e7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./DaBERTo\",\n",
    "    tokenizer=\"./DaBERTo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd17cad4-d2d0-4687-bc45-e8e72b5802ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The sun <mask>.\n",
    "fill_mask(\"Solen er så <mask>.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "experimentId": "2088331440452327",
    "pythonIndentUnit": 2
   },
   "notebookName": "ml_RoBERTa_from_scratch",
   "notebookOrigID": 2088331440452327,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
